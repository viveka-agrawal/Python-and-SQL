# UCI CS275P: Bernoulli Mixture Model for word features 

# Our standard numpy and matplotlib imports
import numpy as np
import matplotlib.pyplot as plt

# Requires pomegranate package implementing EM for mixture models:
# pip3 install pomegranate
# or 
# conda install conda-forge::pomegranate

# Import our custom (fixed) bernoulli object and our Custom General Mixture Model
from general_utils import *
from word_utils import *


def top_ranked_words(model, x, labels, num_words=5):
    """
    Identify the top-ranked words for each cluster in a Bernoulli Mixture Model.

    Args:
        model (CustomGeneralMixtureModel): Trained Bernoulli Mixture Model.
        x (numpy.ndarray): Input data matrix where rows are samples and columns are features.
        labels (numpy.ndarray): Array of word labels corresponding to the features.
        num_words (int, optional): Number of top words to extract per cluster. Defaults to 5.

    Returns:
        list: A 2D list of strings, where each cluster's results are stored as a sublist.
        list[k][n] is the n-th most likely word to have been generated by cluster k.
    """

    # Compute the log prob for all the words
    all_log_probs = model.predict_log_proba(x)  

    # The number of clusters in this model
    num_clusters = all_log_probs.shape[1]

    results = []

    for k in range(num_clusters):

        # sort indices by desc log prob
        ind_desc_log_prob = np.argpartition(all_log_probs[:, k], -num_words)[-num_words:]

        # change indices to labels
        selected_labels = labels[ind_desc_log_prob]

        cluster_results = [str(l[0]) for l in selected_labels]
        results.append(cluster_results)

    return results


def top_ranked_features(model, labels, num_features=5):
    """
    Identify the top-ranked features for each cluster in a Bernoulli Mixture Model.

    Args:
        model (CustomGeneralMixtureModel): Trained Bernoulli Mixture Model.
        labels (numpy.ndarray): Array of feature labels corresponding to the features.
        num_features (int, optional): Number of top features to extract per cluster. Defaults to 5.

    Returns:
        list: A 2D list of strings, where each cluster's results are stored as a sublist.
        list[k][d] is the d-th most likely feature to be observed for cluster k.
    """

    # Extract some stats
    num_clusters = len(model.distributions)
    dims = len(model.distributions[0].probs) 

    # Extract the parameters for each of internal distributions
    # This is to show which features in the cluster have the highest probabilities

    results = []

    # Get the top features and store them
    for k in range(num_clusters):
        # obtain prob vector 
        probs_vector = model.distributions[k].probs

        # obtain indices of features with highest prob
        high_prob_indices = np.argpartition(probs_vector, -num_features)[-num_features:]

        # change indices to labels
        selected_feat_labels = labels[high_prob_indices]

        cluster_results = [str(l[0]) for l in selected_feat_labels]
        results.append(cluster_results)

    return results


def main():
    """
    Main function to fit a Bernoulli Mixture Model, evaluate it, and save results.

    This function performs the following steps:
    1. Loads the dataset and preprocesses it.
    2. Fits a Bernoulli Mixture Model with a specified number of clusters.
    3. Evaluates the model over multiple runs to find the best and worst models.
    4. Extracts and saves the top-ranked words and features for the best and worst models.
    """
    # If we should save the plots or plot them live
    do_save_plots = True

    # Load the data
    data = np.load("wordcat.npy", allow_pickle=True).item()
    x_train = data["Xtrain"]
    x_labels = data["Xlabels"]
    feat_labels = data["featLabels"]

    # Need to subtract 1 since x_train is [1, 2] but we need [0, 1]
    x_train -= 1

    # Some stats
    number_of_dims = x_train.shape[-1]
    #################################################################################
    ## Part A: Fit a Bernoulli Mixture Model with K = 8 clusters
    #################################################################################

    # The number of clusters we wish to use for this part
    number_of_clusters = 8

    # Create the initial components
    components = create_components(x_train, number_of_clusters)

    # Create the Bernoulli Mixture Model
    bmm = CustomGeneralMixtureModel(components, verbose=True, tol=1e-8)
    _, log_probabilities_history = bmm.fit(x_train)

    # Plot the log likelihood vs iteration
    plt.plot(log_probabilities_history)
    plt.xlabel("Iteration")
    plt.ylabel("Log Likelihood")
    plt.tight_layout()

    if(do_save_plots):
        plt.savefig("bmm_PartA_LogLikVsIteration.pdf")
    else:
        plt.show()

    #################################################################################
    ## Part B/C: Fit Bernoulli Mixture Model for M runs
    #################################################################################

    # The number of clusters we wish to use for this part
    number_of_clusters = 8

    models = []
    final_log_probs = []
    for i in range(10):

        # Create the initial components
        components = create_components(x_train, number_of_clusters)

        # Create the Bernoulli Mixture Model
        bmm = CustomGeneralMixtureModel(components, verbose=True, tol=1e-8)
        _, log_probabilities_history = bmm.fit(x_train)

        # Extract the final log_prob
        final_log_prob = log_probabilities_history[-1]

        # Store so we can find the best and worst
        models.append(bmm)
        final_log_probs.append(final_log_prob)

    # find the best Model and get the words/features
    best_model_idx = final_log_probs.index(max(final_log_probs))
    best_model = models[best_model_idx]
    best_words = top_ranked_words(best_model, x_train, x_labels)
    best_features = top_ranked_features(best_model, feat_labels)

    # Format the best words and features with numbers and commas
    best_words_flat = ["{:d}: ".format(i) + ", ".join(sublist) for i, sublist in enumerate(best_words)]
    best_features_flat = ["{:d}: ".format(i) + ", ".join(sublist) for i, sublist in enumerate(best_features)]

    # find the worst Model and get the words/features
    worst_model_idx = final_log_probs.index(min(final_log_probs))
    worst_model = models[worst_model_idx]
    worst_words = top_ranked_words(worst_model, x_train, x_labels)
    worst_features = top_ranked_features(worst_model, feat_labels)

    # Format the worst words and features with numbers and commas
    worst_words_flat = ["{:d}: ".format(i) + ", ".join(sublist) for i, sublist in enumerate(worst_words)]
    worst_features_flat = ["{:d}: ".format(i) + ", ".join(sublist) for i, sublist in enumerate(worst_features)]

    # Save the results to files
    with open("bestRun-words.txt", "w") as f:
        f.write("\n".join(best_words_flat))

    with open("bestRun-features.txt", "w") as f:
        f.write("\n".join(best_features_flat))

    with open("worstRun-words.txt", "w") as f:
        f.write("\n".join(worst_words_flat))

    with open("worstRun-features.txt", "w") as f:
        f.write("\n".join(worst_features_flat))


if __name__ == "__main__":
    main()
