{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "dab386f4-c8e3-4f03-8b23-6dde683901d7",
      "metadata": {
        "id": "dab386f4-c8e3-4f03-8b23-6dde683901d7",
        "outputId": "74bd145e-0e24-491f-c969-eb75435e47d9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting loguru\n",
            "  Downloading loguru-0.7.3-py3-none-any.whl.metadata (22 kB)\n",
            "Downloading loguru-0.7.3-py3-none-any.whl (61 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/61.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.6/61.6 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: loguru\n",
            "Successfully installed loguru-0.7.3\n",
            "Collecting ddddocr\n",
            "  Downloading ddddocr-1.5.6-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from ddddocr) (2.0.2)\n",
            "Collecting onnxruntime (from ddddocr)\n",
            "  Downloading onnxruntime-1.23.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from ddddocr) (11.3.0)\n",
            "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.12/dist-packages (from ddddocr) (4.12.0.88)\n",
            "Collecting coloredlogs (from onnxruntime->ddddocr)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime->ddddocr) (25.9.23)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from onnxruntime->ddddocr) (25.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from onnxruntime->ddddocr) (5.29.5)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from onnxruntime->ddddocr) (1.14.0)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime->ddddocr)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->onnxruntime->ddddocr) (1.3.0)\n",
            "Downloading ddddocr-1.5.6-py3-none-any.whl (75.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.9/75.9 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.23.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (17.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.4/17.4 MB\u001b[0m \u001b[31m110.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: humanfriendly, coloredlogs, onnxruntime, ddddocr\n",
            "Successfully installed coloredlogs-15.0.1 ddddocr-1.5.6 humanfriendly-10.0 onnxruntime-1.23.2\n"
          ]
        }
      ],
      "source": [
        "!pip install loguru\n",
        "!pip install ddddocr"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/Firm.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vz7GRTZcqp4R",
        "outputId": "a7c4b0eb-f0c4-4716-9a97-06ced8349456"
      },
      "id": "vz7GRTZcqp4R",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/Firm.zip\n",
            "  inflating: Firm lists/2025.01.07.xlsx  \n",
            "  inflating: Firm lists/2025.01.04.xlsx  \n",
            "  inflating: Firm lists/2025.01.09.xlsx  \n",
            "  inflating: Firm lists/2025.01.05.xlsx  \n",
            "  inflating: Firm lists/2025.01.08.xlsx  \n",
            "  inflating: Firm lists/2025.01.06.xlsx  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "94d5b705-0b47-4c37-b634-ee5b1500fbd6",
      "metadata": {
        "id": "94d5b705-0b47-4c37-b634-ee5b1500fbd6"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import ddddocr\n",
        "import urllib\n",
        "import time\n",
        "from loguru import logger\n",
        "import pandas as pd\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "from PIL import Image, ImageEnhance, ImageFilter\n",
        "\n",
        "# Configure logger to be less verbose\n",
        "logger.remove()  # Remove default handler\n",
        "logger.add(lambda msg: None, level=\"ERROR\")  # Only show errors\n",
        "\n",
        "def read_excel_from_path(path):\n",
        "    \"\"\"Read excel file and return list of firm names\"\"\"\n",
        "    df = pd.read_excel(path, header=None)\n",
        "    firm_list = df[0].astype(str).to_numpy().tolist()\n",
        "    firm_list = [firm for firm in firm_list if firm and firm != 'nan']\n",
        "    return firm_list\n",
        "\n",
        "def preprocess_captcha_image(image_path):\n",
        "    \"\"\"Preprocess CAPTCHA image to improve OCR accuracy\"\"\"\n",
        "    img = Image.open(image_path)\n",
        "    img = img.convert('L')\n",
        "    enhancer = ImageEnhance.Contrast(img)\n",
        "    img = enhancer.enhance(2.0)\n",
        "    img = img.filter(ImageFilter.SHARPEN)\n",
        "    threshold = 128\n",
        "    img = img.point(lambda p: 255 if p > threshold else 0)\n",
        "    img.save('response_processed.jpeg')\n",
        "    return 'response_processed.jpeg'\n",
        "\n",
        "def validate_captcha_format(captcha_text):\n",
        "    \"\"\"Validate CAPTCHA text format\"\"\"\n",
        "    if not captcha_text:\n",
        "        return False\n",
        "    captcha_text = captcha_text.strip()\n",
        "    if len(captcha_text) < 3 or len(captcha_text) > 8:\n",
        "        return False\n",
        "    if not captcha_text.isalnum():\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "def sc(start, end, sessions, cookies, firm_list):\n",
        "    \"\"\"Enhanced main scraping function with superior network resilience\"\"\"\n",
        "\n",
        "    print(\"Total number of firms to search:\", len(firm_list))\n",
        "    output = []\n",
        "    firm_captured, firm_not_found, firm_errors = [], [], []\n",
        "\n",
        "    cookie = 'JSESSIONID=' + str(sessions) + \"; insert_cookie=\" + str(cookies)\n",
        "\n",
        "    # Initialize OCR once\n",
        "    ocr = ddddocr.DdddOcr(show_ad=False)\n",
        "\n",
        "    def safe_request(func_name, request_func, max_network_retries=5):\n",
        "        \"\"\"\n",
        "        Wrapper for all network requests with exponential backoff\n",
        "        Retries on network errors but not on application-level failures\n",
        "        \"\"\"\n",
        "        for attempt in range(max_network_retries):\n",
        "            try:\n",
        "                result = request_func()\n",
        "                return result, True\n",
        "            except (requests.exceptions.ConnectionError,\n",
        "                    requests.exceptions.Timeout,\n",
        "                    requests.exceptions.ChunkedEncodingError) as e:\n",
        "                wait_time = min(2 ** attempt, 30)  # Exponential backoff, max 30s\n",
        "                if attempt < max_network_retries - 1:\n",
        "                    logger.warning(f\"{func_name} network error (attempt {attempt+1}/{max_network_retries}): {type(e).__name__}. Retrying in {wait_time}s...\")\n",
        "                    time.sleep(wait_time)\n",
        "                else:\n",
        "                    logger.error(f\"{func_name} failed after {max_network_retries} attempts: {e}\")\n",
        "                    return None, False\n",
        "            except Exception as e:\n",
        "                logger.error(f\"{func_name} unexpected error: {e}\")\n",
        "                return None, False\n",
        "        return None, False\n",
        "\n",
        "    def get_image(cookie):\n",
        "        \"\"\"Download CAPTCHA image with network resilience\"\"\"\n",
        "        def _download():\n",
        "            now_time = int(round(time.time() * 1000))\n",
        "            url = f\"https://wzxxbg.mofcom.gov.cn/gspt/infoPub/entp/search/vCode?r={now_time}\"\n",
        "\n",
        "            headers = {\n",
        "                'Accept': 'image/avif,image/webp,image/apng,image/svg+xml,image/*,*/*;q=0.8',\n",
        "                'Accept-Language': 'zh,zh-CN;q=0.9,en;q=0.8',\n",
        "                'Cache-Control': 'no-cache',\n",
        "                'Connection': 'keep-alive',\n",
        "                'Cookie': cookie,\n",
        "                'Pragma': 'no-cache',\n",
        "                'Referer': 'https://wzxxbg.mofcom.gov.cn/gspt/vCode.html',\n",
        "                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.0.0 Safari/537.36',\n",
        "            }\n",
        "\n",
        "            response = requests.get(url, headers=headers, timeout=15)\n",
        "\n",
        "            if response.status_code == 200 and len(response.content) > 0:\n",
        "                with open('response.jpeg', 'wb') as photo:\n",
        "                    photo.write(response.content)\n",
        "                return True\n",
        "            return False\n",
        "\n",
        "        result, success = safe_request(\"get_image\", _download, max_network_retries=3)\n",
        "        return result if success else False\n",
        "\n",
        "    def get_verify(image_code, cookie):\n",
        "        \"\"\"Verify CAPTCHA code with network resilience\"\"\"\n",
        "        def _verify():\n",
        "            url = \"https://wzxxbg.mofcom.gov.cn/gspt/infoPub/entp/search/checkVCode\"\n",
        "            payload = f\"searchWzCode={image_code}\"\n",
        "            headers = {\n",
        "                'Accept': '*/*',\n",
        "                'Accept-Language': 'zh,zh-CN;q=0.9,en;q=0.8',\n",
        "                'Content-Type': 'application/x-www-form-urlencoded; charset=UTF-8',\n",
        "                'Cookie': cookie,\n",
        "                'Origin': 'https://wzxxbg.mofcom.gov.cn',\n",
        "                'Referer': 'https://wzxxbg.mofcom.gov.cn/gspt/vCode.html',\n",
        "                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.0.0 Safari/537.36',\n",
        "                'X-Requested-With': 'XMLHttpRequest',\n",
        "            }\n",
        "\n",
        "            response = requests.post(url, headers=headers, data=payload, timeout=15)\n",
        "            return response.json()\n",
        "\n",
        "        result, success = safe_request(\"get_verify\", _verify, max_network_retries=4)\n",
        "        return result if success else {'status': 3}\n",
        "\n",
        "    def get_search_company(company, image_code, cookie):\n",
        "        \"\"\"Search for company with network resilience\"\"\"\n",
        "        def _search():\n",
        "            url = \"https://wzxxbg.mofcom.gov.cn/gspt/infoPub/entp/search/searchEntpList\"\n",
        "            name = urllib.parse.quote(company)\n",
        "            payload = f\"keyWord={name}&searchWzCode={image_code}\"\n",
        "            headers = {\n",
        "                'Accept': '*/*',\n",
        "                'Accept-Language': 'zh,zh-CN;q=0.9,en;q=0.8',\n",
        "                'Content-Type': 'application/x-www-form-urlencoded; charset=UTF-8',\n",
        "                'Cookie': cookie,\n",
        "                'Origin': 'https://wzxxbg.mofcom.gov.cn',\n",
        "                'Referer': 'https://wzxxbg.mofcom.gov.cn/gspt/vCode.html',\n",
        "                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.0.0 Safari/537.36',\n",
        "                'X-Requested-With': 'XMLHttpRequest',\n",
        "            }\n",
        "\n",
        "            response = requests.post(url, headers=headers, data=payload, timeout=20)\n",
        "\n",
        "            if response.status_code == 200:\n",
        "                res_data = response.json()\n",
        "                data = res_data.get('data')\n",
        "                if data:\n",
        "                    wzResult = data.get('wzResult')\n",
        "                    result_list = wzResult.get('result')\n",
        "                    if result_list and len(result_list) > 0:\n",
        "                        token = result_list[0].get('TOKEN')\n",
        "                        entp_id = result_list[0].get('ENTP_MAIN_ID')\n",
        "                        return [token, entp_id, company]\n",
        "                    else:\n",
        "                        firm_not_found.append(company)\n",
        "                        logger.info(f'Firm not found: {company}')\n",
        "                else:\n",
        "                    firm_not_found.append(company)\n",
        "                    logger.info(f'No data returned for: {company}')\n",
        "            else:\n",
        "                logger.error(f'Search failed for {company}: {response.status_code}')\n",
        "            return []\n",
        "\n",
        "        result, success = safe_request(\"get_search_company\", _search, max_network_retries=5)\n",
        "        if not success:\n",
        "            firm_errors.append(company)\n",
        "            return []\n",
        "        return result if result else []\n",
        "\n",
        "    def image_to_str():\n",
        "        \"\"\"Enhanced OCR with preprocessing\"\"\"\n",
        "        try:\n",
        "            processed_path = preprocess_captcha_image('response.jpeg')\n",
        "            results = []\n",
        "\n",
        "            # OCR on processed image\n",
        "            with open(processed_path, 'rb') as f:\n",
        "                img_bytes = f.read()\n",
        "                res1 = ocr.classification(img_bytes)\n",
        "                if validate_captcha_format(res1):\n",
        "                    results.append(res1)\n",
        "\n",
        "            # OCR on original image\n",
        "            with open('response.jpeg', 'rb') as f:\n",
        "                img_bytes = f.read()\n",
        "                res2 = ocr.classification(img_bytes)\n",
        "                if validate_captcha_format(res2):\n",
        "                    results.append(res2)\n",
        "\n",
        "            if results:\n",
        "                return max(set(results), key=results.count)\n",
        "\n",
        "            return res1 if res1 else res2\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"OCR failed: {e}\")\n",
        "            return \"\"\n",
        "\n",
        "    def get_company_detail(entp_id, token, company_name):\n",
        "        \"\"\"Retrieve detailed company information with network resilience\"\"\"\n",
        "        def _get_detail():\n",
        "            url = \"https://wzxxbg.mofcom.gov.cn/gspt/infoPub/entp/search/wzEntpDetail\"\n",
        "            payload = f\"entpId={entp_id}&token={token}\"\n",
        "            headers = {\n",
        "                'Accept': '*/*',\n",
        "                'Accept-Language': 'zh,zh-CN;q=0.9,en;q=0.8',\n",
        "                'Content-Type': 'application/x-www-form-urlencoded; charset=UTF-8',\n",
        "                'Cookie': cookie,\n",
        "                'Origin': 'https://wzxxbg.mofcom.gov.cn',\n",
        "                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.0.0 Safari/537.36',\n",
        "                'X-Requested-With': 'XMLHttpRequest',\n",
        "            }\n",
        "\n",
        "            response = requests.post(url, headers=headers, data=payload, timeout=20)\n",
        "\n",
        "            if response.status_code == 200:\n",
        "                resp_json = response.json()\n",
        "                data = resp_json.get('data')\n",
        "                wzResult = data.get('wzResult')\n",
        "\n",
        "                res_data = {}\n",
        "                res_data[\"search_name\"] = company_name\n",
        "                res_data[\"entp_name\"] = wzResult.get('ENTP_NAME', 'N/A')\n",
        "                res_data[\"gs_status_name\"] = wzResult.get('GS_STATUS_NAME', 'N/A')\n",
        "                res_data[\"business_scope\"] = wzResult.get('BUSINESS_SCOPE', 'N/A')\n",
        "                res_data[\"entp_gs_code\"] = wzResult.get(\"ENTP_GS_CODE\", 'N/A')\n",
        "                res_data[\"recorddatefmt\"] = wzResult.get(\"RECORDDATEFMT\", 'N/A')\n",
        "                res_data[\"industryname\"] = wzResult.get(\"INDUSTRYNAME\", 'N/A')\n",
        "                res_data[\"register_capital\"] = wzResult.get(\"REGISTER_CAPITAL\", 'N/A')\n",
        "                res_data[\"unit_name\"] = wzResult.get(\"UNITNAME\", 'N/A')\n",
        "                res_data[\"reg_addr\"] = wzResult.get(\"REG_ADDR\", 'N/A')\n",
        "                res_data[\"right_man\"] = wzResult.get(\"RIGHT_MAN\", 'N/A')\n",
        "\n",
        "                # Investor information\n",
        "                investorResult = data.get('investorResult', [])\n",
        "                investorResult = [ir for ir in investorResult if ir.get('INVESTOR_NAME') is not None]\n",
        "                investors = []\n",
        "                for inv in investorResult:\n",
        "                    iName = inv.get('INVESTOR_NAME', 'N/A')\n",
        "                    cName = inv.get('COUNTRYNAME', 'N/A')\n",
        "                    amount = inv.get('CAPITAL_AMOUNT', 'N/A')\n",
        "                    investors.append(f\"{iName}~{cName}~{amount}\")\n",
        "                res_data[\"investor_info\"] = '|'.join(investors) if investors else 'N/A'\n",
        "\n",
        "                # Change history\n",
        "                entpAlterList = data.get('entpAlterList', [])\n",
        "                entpAlterList = [ir for ir in entpAlterList if ir.get('ALTITEM') is not None]\n",
        "                changes = []\n",
        "                for change in entpAlterList:\n",
        "                    cType = change.get('ALTITEM', 'N/A')\n",
        "                    cBefore = change.get('ALTBE', 'N/A')\n",
        "                    cAfter = change.get('ALTAF', 'N/A')\n",
        "                    cDate = change.get('ALTDATE', 'N/A')\n",
        "                    changes.append(f\"{cType}~{cBefore}~{cAfter}~{cDate}\")\n",
        "                res_data[\"changes_info\"] = '|'.join(changes) if changes else 'N/A'\n",
        "\n",
        "                # Annual reports\n",
        "                lhnbResult = data.get('lhnbResult', [])\n",
        "                if lhnbResult:\n",
        "                    res_data['year_report'] = \"|\".join([r.get('YEAR', '') for r in lhnbResult])\n",
        "                else:\n",
        "                    res_data['year_report'] = 'N/A'\n",
        "\n",
        "                return res_data\n",
        "            return None\n",
        "\n",
        "        result, success = safe_request(\"get_company_detail\", _get_detail, max_network_retries=5)\n",
        "        return result if success else None\n",
        "\n",
        "    def try_catch_loop(cookie, start, end):\n",
        "        \"\"\"Main processing loop with enhanced resilience\"\"\"\n",
        "        time_to_stop = False\n",
        "        max_retries_for_status3 = 10  # Increased from 8\n",
        "        consecutive_failures = 0\n",
        "        max_consecutive_failures = 15  # Stop if 15 firms fail in a row\n",
        "\n",
        "        for i in tqdm(range(start, end), desc=\"Processing firms\", colour='green'):\n",
        "            if time_to_stop:\n",
        "                break\n",
        "\n",
        "            # Adaptive delay based on consecutive failures\n",
        "            if consecutive_failures > 5:\n",
        "                delay = random.uniform(4, 7)  # Longer delay if having issues\n",
        "            else:\n",
        "                delay = random.uniform(2, 4)  # Normal delay\n",
        "            time.sleep(delay)\n",
        "\n",
        "            retries = 0\n",
        "            captcha_success = False\n",
        "            firm_processed = False\n",
        "\n",
        "            while retries < max_retries_for_status3:\n",
        "                # Download CAPTCHA\n",
        "                if not get_image(cookie):\n",
        "                    retries += 1\n",
        "                    time.sleep(3)\n",
        "                    continue\n",
        "\n",
        "                # OCR\n",
        "                image_code = image_to_str()\n",
        "\n",
        "                if not image_code or not validate_captcha_format(image_code):\n",
        "                    retries += 1\n",
        "                    continue\n",
        "\n",
        "                # Verify CAPTCHA\n",
        "                verify_data = get_verify(image_code, cookie)\n",
        "\n",
        "                if int(verify_data.get('status', 3)) == 3:\n",
        "                    retries += 1\n",
        "                    time.sleep(1)\n",
        "                    continue\n",
        "\n",
        "                # CAPTCHA successful\n",
        "                captcha_success = True\n",
        "                token_list = get_search_company(\n",
        "                    company=firm_list[i],\n",
        "                    image_code=image_code,\n",
        "                    cookie=cookie\n",
        "                )\n",
        "\n",
        "                # Retrieve details if found\n",
        "                if token_list and len(token_list) == 3:\n",
        "                    entp_id = token_list[1]\n",
        "                    token = token_list[0]\n",
        "                    company_name = token_list[2]\n",
        "\n",
        "                    data = get_company_detail(entp_id=entp_id, token=token, company_name=company_name)\n",
        "                    if data:\n",
        "                        output.append(data)\n",
        "                        firm_captured.append(company_name)\n",
        "                        consecutive_failures = 0  # Reset counter on success\n",
        "                        firm_processed = True\n",
        "                    else:\n",
        "                        consecutive_failures += 1\n",
        "                else:\n",
        "                    # Firm not found is not a failure\n",
        "                    consecutive_failures = 0\n",
        "                    firm_processed = True\n",
        "\n",
        "                break\n",
        "\n",
        "            if not firm_processed:\n",
        "                consecutive_failures += 1\n",
        "\n",
        "            # Check for cookie expiration or persistent issues\n",
        "            if retries >= max_retries_for_status3:\n",
        "                logger.error(f\"Max CAPTCHA retries for firm {i}: {firm_list[i]}\")\n",
        "                if not captcha_success:\n",
        "                    time_to_stop = True\n",
        "                    print(\"Cookie likely expired - please refresh\")\n",
        "                    return True\n",
        "\n",
        "            if consecutive_failures >= max_consecutive_failures:\n",
        "                logger.error(f\"Too many consecutive failures ({consecutive_failures}). Network may be unstable.\")\n",
        "                print(f\"Stopping due to {consecutive_failures} consecutive failures. Check network connection.\")\n",
        "                return True\n",
        "\n",
        "        return False\n",
        "\n",
        "    # Main execution\n",
        "    start = start\n",
        "    end = min(len(firm_list), end)\n",
        "    max_retries = 2  # Reduced outer retries since inner retries are stronger\n",
        "\n",
        "    for retry_attempt in range(max_retries):\n",
        "        if len(firm_captured) == 0:\n",
        "            s = start\n",
        "        else:\n",
        "            last_firm = firm_captured[-1]\n",
        "            try:\n",
        "                last_index = firm_list.index(last_firm)\n",
        "                s = last_index + 1\n",
        "            except ValueError:\n",
        "                s = start\n",
        "\n",
        "        if s >= end:\n",
        "            logger.info(\"All firms in range have been processed\")\n",
        "            break\n",
        "\n",
        "        logger.info(f\"Starting/resuming from firm index {s}\")\n",
        "\n",
        "        try:\n",
        "            cookie_expired = try_catch_loop(cookie, s, end)\n",
        "            if cookie_expired:\n",
        "                logger.warning(\"Cookie expired or network unstable - stopping this chunk\")\n",
        "                break\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Unexpected error: {e}\")\n",
        "            time.sleep(15)\n",
        "            print(f\"Captured: {len(output)}, Not found: {len(firm_not_found)}, Errors: {len(firm_errors)}\")\n",
        "\n",
        "    # Generate results\n",
        "    if output:\n",
        "        result_dic = {\n",
        "            '搜索公司名称': [record.get('search_name', 'N/A') for record in output],\n",
        "            '公司名称': [record.get('entp_name', 'N/A') for record in output],\n",
        "            '统一社会信用代码/组织机构代码': [record.get('entp_gs_code', 'N/A') for record in output],\n",
        "            '状况': [record.get('gs_status_name', 'N/A') for record in output],\n",
        "            '成立日期': [record.get('recorddatefmt', 'N/A') for record in output],\n",
        "            '投资行业': [record.get('industryname', 'N/A') for record in output],\n",
        "            '注册资本': [record.get('register_capital', 'N/A') for record in output],\n",
        "            '注册资本单位': [record.get('unit_name', 'N/A') for record in output],\n",
        "            '经营范围': [record.get('business_scope', 'N/A') for record in output],\n",
        "            '地址': [record.get('reg_addr', 'N/A') for record in output],\n",
        "            '法定代表人': [record.get('right_man', 'N/A') for record in output],\n",
        "            '投资者信息': [record.get('investor_info', 'N/A') for record in output],\n",
        "            '变更信息': [record.get('changes_info', 'N/A') for record in output],\n",
        "            '年报年度': [record.get('year_report', 'N/A') for record in output]\n",
        "        }\n",
        "        result = pd.DataFrame.from_dict(result_dic)\n",
        "    else:\n",
        "        result = pd.DataFrame()\n",
        "\n",
        "    # Log summary\n",
        "    success_rate = (len(output) / (end - start)) * 100 if (end - start) > 0 else 0\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"Summary:\")\n",
        "    print(f\"Successfully captured: {len(output)}\")\n",
        "    print(f\"Not found: {len(firm_not_found)}\")\n",
        "    print(f\"Errors: {len(firm_errors)}\")\n",
        "    print(f\"Success rate: {success_rate:.1f}%\")\n",
        "    print(f\"{'='*50}\\n\")\n",
        "\n",
        "    return result\n",
        "\n",
        "def processing(start, end, session, cookie, path_to_firm_list, max_chunk_size):\n",
        "    \"\"\"Process firms in chunks with progress tracking\"\"\"\n",
        "    firm_list = read_excel_from_path(path_to_firm_list)\n",
        "\n",
        "    if start is None:\n",
        "        start = 0\n",
        "    if end is None:\n",
        "        end = len(firm_list)\n",
        "\n",
        "    print(f\"Total firms to process: {end - start}\")\n",
        "    print(f\"Processing in chunks of: {max_chunk_size}\")\n",
        "\n",
        "    total_captured = 0\n",
        "    total_not_found = 0\n",
        "    total_errors = 0\n",
        "\n",
        "    for current_start in range(start, end, max_chunk_size):\n",
        "        current_end = min(current_start + max_chunk_size, end)\n",
        "\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"Processing chunk: {current_start+1} to {current_end}\")\n",
        "        print(f\"{'='*60}\\n\")\n",
        "\n",
        "        result = sc(current_start, current_end, session, cookie, firm_list)\n",
        "\n",
        "        if not result.empty:\n",
        "            filename = f\"./firm_info_{current_start+1}_{current_end}.xlsx\"\n",
        "            result.to_excel(filename, index=False)\n",
        "            print(f\"Saved: {filename}\")\n",
        "            total_captured += len(result)\n",
        "        else:\n",
        "            print(f\"No results for chunk {current_start+1}-{current_end}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"FINAL SUMMARY:\")\n",
        "    print(f\"Total firms captured: {total_captured}/{end - start}\")\n",
        "    #print(f\"Number of firms tested on: {end}\")\n",
        "    print(f\"Overall Accuracy: { (total_captured / (end - start)) * 100:.2f}%\")\n",
        "    print(f\"Processing complete!\")\n",
        "    print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "def classify_fie_probability(firm_list_path, output_path=None):\n",
        "    \"\"\"\n",
        "    Classify firms by probability of being Foreign Invested Enterprises\n",
        "    Returns firms sorted by likelihood of being in MOFCOM database\n",
        "    \"\"\"\n",
        "\n",
        "    # Read firm list\n",
        "    df = pd.read_excel(firm_list_path, header=None)\n",
        "    df.columns = ['firm_name']\n",
        "    df['firm_name'] = df['firm_name'].astype(str)\n",
        "    df = df[df['firm_name'] != 'nan']\n",
        "\n",
        "    # Initialize probability score\n",
        "    df['fie_score'] = 0\n",
        "    df['indicators'] = ''\n",
        "\n",
        "    # High probability indicators (foreign company patterns)\n",
        "    foreign_patterns = {\n",
        "        'foreign_name': r'[A-Z][a-z]+ [A-Z][a-z]+|[A-Z]{2,}',  # English names\n",
        "        'foreign_suffix': r'\\(.*\\)$|（.*）$',  # Company with foreign parent in parentheses\n",
        "        'ltd_variants': r'(?i)(pty|pte|ltd|inc|corp|gmbh|sarl|s\\.a\\.|limited)\\.?$',\n",
        "        'location_hk': r'(?i)(香港|hk|hong kong)',\n",
        "        'location_taiwan': r'(?i)(台湾|taiwan)',\n",
        "        'location_foreign': r'(?i)(新加坡|singapore|日本|japan|韩国|korea|美国|usa|德国|germany)',\n",
        "    }\n",
        "\n",
        "    # Medium probability indicators\n",
        "    foreign_keywords = {\n",
        "        'foreign_region': r'(?i)(外商|外资)',  # \"Foreign investment\" in Chinese\n",
        "        'intl_keywords': r'(?i)(国际|international)',\n",
        "        'global_keywords': r'(?i)(环球|global|worldwide)',\n",
        "    }\n",
        "\n",
        "    # Low probability indicators (domestic-only patterns)\n",
        "    domestic_patterns = {\n",
        "        'province': r'^(北京|上海|天津|重庆|河北|山西|辽宁|吉林|黑龙江|江苏|浙江|安徽|福建|江西|山东|河南|湖北|湖南|广东|海南|四川|贵州|云南|陕西|甘肃|青海|台湾|内蒙古|广西|西藏|宁夏|新疆)',\n",
        "        'city_start': r'^[\\u4e00-\\u9fa5]{2,3}市',  # Starts with Chinese city name\n",
        "    }\n",
        "\n",
        "    def calculate_score(name):\n",
        "        score = 0\n",
        "        indicators = []\n",
        "\n",
        "        # Check foreign patterns (high confidence)\n",
        "        for pattern_name, pattern in foreign_patterns.items():\n",
        "            if re.search(pattern, name):\n",
        "                if pattern_name == 'foreign_name':\n",
        "                    score += 30\n",
        "                    indicators.append('英文名')\n",
        "                elif pattern_name == 'location_hk':\n",
        "                    score += 25\n",
        "                    indicators.append('香港')\n",
        "                elif pattern_name == 'location_foreign':\n",
        "                    score += 40\n",
        "                    indicators.append('外国地名')\n",
        "                else:\n",
        "                    score += 20\n",
        "                    indicators.append(pattern_name)\n",
        "\n",
        "        # Check foreign keywords (medium confidence)\n",
        "        for keyword_name, pattern in foreign_keywords.items():\n",
        "            if re.search(pattern, name):\n",
        "                score += 15\n",
        "                indicators.append(keyword_name)\n",
        "\n",
        "        # Unified credit codes starting with 9 are usually newer companies\n",
        "        if name.startswith('91'):\n",
        "            score += 5\n",
        "            indicators.append('统一信用代码')\n",
        "\n",
        "        # Check domestic patterns (reduce score)\n",
        "        for pattern_name, pattern in domestic_patterns.items():\n",
        "            if re.search(pattern, name):\n",
        "                score -= 10\n",
        "                indicators.append(f'domestic_{pattern_name}')\n",
        "\n",
        "        # Pure numeric codes (might be organization codes for FIEs)\n",
        "        if re.match(r'^\\d{9,18}[A-Z0-9]?$', name):\n",
        "            score += 10\n",
        "            indicators.append('组织代码')\n",
        "\n",
        "        return score, ', '.join(indicators) if indicators else 'none'\n",
        "\n",
        "    # Calculate scores\n",
        "    df[['fie_score', 'indicators']] = df['firm_name'].apply(\n",
        "        lambda x: pd.Series(calculate_score(x))\n",
        "    )\n",
        "\n",
        "    # Classify by probability\n",
        "    def classify(score):\n",
        "        if score >= 40:\n",
        "            return 'Very High'\n",
        "        elif score >= 25:\n",
        "            return 'High'\n",
        "        elif score >= 10:\n",
        "            return 'Medium'\n",
        "        elif score >= 0:\n",
        "            return 'Low'\n",
        "        else:\n",
        "            return 'Very Low'\n",
        "\n",
        "    df['probability'] = df['fie_score'].apply(classify)\n",
        "\n",
        "    # Sort by probability (highest first)\n",
        "    df_sorted = df.sort_values('fie_score', ascending=False)\n",
        "\n",
        "    # Statistics\n",
        "    stats = df['probability'].value_counts()\n",
        "    total = len(df)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"FIE PROBABILITY ANALYSIS\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"Total firms analyzed: {total}\")\n",
        "    print(\"\\nProbability Distribution:\")\n",
        "    for prob in ['Very High', 'High', 'Medium', 'Low', 'Very Low']:\n",
        "        count = stats.get(prob, 0)\n",
        "        pct = (count/total)*100\n",
        "        print(f\"  {prob:12s}: {count:4d} firms ({pct:5.1f}%)\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"RECOMMENDATION:\")\n",
        "    high_prob = stats.get('Very High', 0) + stats.get('High', 0)\n",
        "    expected_success = high_prob * 0.4  # Assume 40% of high-prob firms exist\n",
        "    print(f\"Focus on 'Very High' and 'High' probability firms: {high_prob} firms\")\n",
        "    print(f\"Expected successful captures: ~{int(expected_success)} firms\")\n",
        "    print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "    # Sample output\n",
        "    print(\"Sample High Probability Firms:\")\n",
        "    print(df_sorted.head(10)[['firm_name', 'fie_score', 'probability', 'indicators']].to_string(index=False))\n",
        "\n",
        "    print(\"\\n\\nSample Low Probability Firms:\")\n",
        "    print(df_sorted.tail(10)[['firm_name', 'fie_score', 'probability', 'indicators']].to_string(index=False))\n",
        "\n",
        "    # Save if output path provided\n",
        "    if output_path:\n",
        "        df_sorted.to_excel(output_path, index=False)\n",
        "        print(f\"\\n\\nFull analysis saved to: {output_path}\")\n",
        "\n",
        "    return df_sorted\n",
        "\n",
        "# Example usage:\n",
        "# classified_df = classify_fie_probability('/path/to/your/firmlist.xlsx', 'classified_firms.xlsx')\n",
        "#\n",
        "# To scrape only high-probability firms:\n",
        "# high_prob_firms = classified_df[classified_df['probability'].isin(['Very High', 'High'])]\n",
        "# high_prob_firms[['firm_name']].to_excel('high_prob_only.xlsx', index=False, header=False)\n"
      ],
      "metadata": {
        "id": "SEVpIc1U5lbd"
      },
      "id": "SEVpIc1U5lbd",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classified_df = classify_fie_probability('/content/Firm lists/2025.01.09.xlsx', 'classified_firms.xlsx')\n",
        "\n",
        "# To scrape only high-probability firms:\n",
        "high_prob_firms = classified_df[classified_df['probability'].isin(['Very High', 'High'])]\n",
        "high_prob_firms[['firm_name']].to_excel('high_prob_only.xlsx', index=False, header=False)\n",
        "\n",
        "processing(0, 2000, 'CA1711D9EA2AD638C4265228A73BF5DE', '32151754', 'high_prob_only.xlsx', 100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FkjrxgFUotGP",
        "outputId": "402d3848-bf50-402a-f839-60c8cf8e55a6"
      },
      "id": "FkjrxgFUotGP",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "FIE PROBABILITY ANALYSIS\n",
            "============================================================\n",
            "Total firms analyzed: 21090\n",
            "\n",
            "Probability Distribution:\n",
            "  Very High   :   58 firms (  0.3%)\n",
            "  High        : 10808 firms ( 51.2%)\n",
            "  Medium      : 7567 firms ( 35.9%)\n",
            "  Low         : 2256 firms ( 10.7%)\n",
            "  Very Low    :  401 firms (  1.9%)\n",
            "\n",
            "============================================================\n",
            "RECOMMENDATION:\n",
            "Focus on 'Very High' and 'High' probability firms: 10866 firms\n",
            "Expected successful captures: ~4346 firms\n",
            "============================================================\n",
            "\n",
            "Sample High Probability Firms:\n",
            "         firm_name  fie_score probability      indicators\n",
            "91440300MA5HKQYK1X         60   Very High 英文名, 香港, 统一信用代码\n",
            "91440400MA55UHKB8E         60   Very High 英文名, 香港, 统一信用代码\n",
            "91310115MADFHKJK6C         60   Very High 英文名, 香港, 统一信用代码\n",
            "91440300MA5HKE439A         60   Very High 英文名, 香港, 统一信用代码\n",
            "91331000MA7DHK4N21         60   Very High 英文名, 香港, 统一信用代码\n",
            "91440300MA5HKJGQ7T         60   Very High 英文名, 香港, 统一信用代码\n",
            "91440300MA5HK8344T         60   Very High 英文名, 香港, 统一信用代码\n",
            "91310000MA1HK0H36A         60   Very High 英文名, 香港, 统一信用代码\n",
            "91210213MA0YW2HKX0         60   Very High 英文名, 香港, 统一信用代码\n",
            "91440113MACCHKDE4W         60   Very High 英文名, 香港, 统一信用代码\n",
            "\n",
            "\n",
            "Sample Low Probability Firms:\n",
            "      firm_name  fie_score probability                             indicators\n",
            "   天津艺虹印刷发展有限公司        -10    Very Low                      domestic_province\n",
            "      义乌市金肯贸易商行        -10    Very Low                    domestic_city_start\n",
            "   上海斗牛士牛排馆有限公司        -10    Very Low                      domestic_province\n",
            "   上海静湖酒业有限责任公司        -10    Very Low                      domestic_province\n",
            " 福建省福新房地产开发有限公司        -10    Very Low                      domestic_province\n",
            " 佛山市达米克医疗用品有限公司        -10    Very Low                    domestic_city_start\n",
            "    清远市尚霖布艺有限公司        -10    Very Low                    domestic_city_start\n",
            "   广东华源盛业贸易有限公司        -10    Very Low                      domestic_province\n",
            "重庆市梁平县宏美达纺织有限公司        -20    Very Low domestic_province, domestic_city_start\n",
            " 重庆市永川区积流贸易有限公司        -20    Very Low domestic_province, domestic_city_start\n",
            "\n",
            "\n",
            "Full analysis saved to: classified_firms.xlsx\n",
            "Total firms to process: 2000\n",
            "Processing in chunks of: 100\n",
            "\n",
            "============================================================\n",
            "Processing chunk: 1 to 100\n",
            "============================================================\n",
            "\n",
            "Total number of firms to search: 10866\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing firms: 100%|\u001b[32m██████████\u001b[0m| 100/100 [12:26<00:00,  7.46s/it]\n",
            "Processing firms: 100%|\u001b[32m██████████\u001b[0m| 10/10 [00:59<00:00,  6.00s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "Summary:\n",
            "Successfully captured: 10\n",
            "Not found: 100\n",
            "Errors: 0\n",
            "Success rate: 10.0%\n",
            "==================================================\n",
            "\n",
            "Saved: ./firm_info_1_100.xlsx\n",
            "\n",
            "============================================================\n",
            "Processing chunk: 101 to 200\n",
            "============================================================\n",
            "\n",
            "Total number of firms to search: 10866\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing firms: 100%|\u001b[32m██████████\u001b[0m| 100/100 [12:58<00:00,  7.79s/it]\n",
            "Processing firms: 100%|\u001b[32m██████████\u001b[0m| 16/16 [01:40<00:00,  6.29s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "Summary:\n",
            "Successfully captured: 7\n",
            "Not found: 109\n",
            "Errors: 0\n",
            "Success rate: 7.0%\n",
            "==================================================\n",
            "\n",
            "Saved: ./firm_info_101_200.xlsx\n",
            "\n",
            "============================================================\n",
            "Processing chunk: 201 to 300\n",
            "============================================================\n",
            "\n",
            "Total number of firms to search: 10866\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing firms: 100%|\u001b[32m██████████\u001b[0m| 100/100 [11:35<00:00,  6.95s/it]\n",
            "Processing firms: 100%|\u001b[32m██████████\u001b[0m| 2/2 [00:12<00:00,  6.42s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "Summary:\n",
            "Successfully captured: 8\n",
            "Not found: 94\n",
            "Errors: 0\n",
            "Success rate: 8.0%\n",
            "==================================================\n",
            "\n",
            "Saved: ./firm_info_201_300.xlsx\n",
            "\n",
            "============================================================\n",
            "Processing chunk: 301 to 400\n",
            "============================================================\n",
            "\n",
            "Total number of firms to search: 10866\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing firms: 100%|\u001b[32m██████████\u001b[0m| 100/100 [13:37<00:00,  8.17s/it]\n",
            "Processing firms: 100%|\u001b[32m██████████\u001b[0m| 2/2 [00:12<00:00,  6.46s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "Summary:\n",
            "Successfully captured: 10\n",
            "Not found: 92\n",
            "Errors: 0\n",
            "Success rate: 10.0%\n",
            "==================================================\n",
            "\n",
            "Saved: ./firm_info_301_400.xlsx\n",
            "\n",
            "============================================================\n",
            "Processing chunk: 401 to 500\n",
            "============================================================\n",
            "\n",
            "Total number of firms to search: 10866\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing firms: 100%|\u001b[32m██████████\u001b[0m| 100/100 [12:22<00:00,  7.43s/it]\n",
            "Processing firms: 100%|\u001b[32m██████████\u001b[0m| 1/1 [00:05<00:00,  5.35s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "Summary:\n",
            "Successfully captured: 14\n",
            "Not found: 87\n",
            "Errors: 0\n",
            "Success rate: 14.0%\n",
            "==================================================\n",
            "\n",
            "Saved: ./firm_info_401_500.xlsx\n",
            "\n",
            "============================================================\n",
            "Processing chunk: 501 to 600\n",
            "============================================================\n",
            "\n",
            "Total number of firms to search: 10866\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing firms: 100%|\u001b[32m██████████\u001b[0m| 100/100 [13:18<00:00,  7.98s/it]\n",
            "Processing firms: 100%|\u001b[32m██████████\u001b[0m| 4/4 [00:26<00:00,  6.58s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "Summary:\n",
            "Successfully captured: 15\n",
            "Not found: 89\n",
            "Errors: 0\n",
            "Success rate: 15.0%\n",
            "==================================================\n",
            "\n",
            "Saved: ./firm_info_501_600.xlsx\n",
            "\n",
            "============================================================\n",
            "Processing chunk: 601 to 700\n",
            "============================================================\n",
            "\n",
            "Total number of firms to search: 10866\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing firms: 100%|\u001b[32m██████████\u001b[0m| 100/100 [12:20<00:00,  7.40s/it]\n",
            "Processing firms: 100%|\u001b[32m██████████\u001b[0m| 22/22 [02:59<00:00,  8.15s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "Summary:\n",
            "Successfully captured: 8\n",
            "Not found: 114\n",
            "Errors: 0\n",
            "Success rate: 8.0%\n",
            "==================================================\n",
            "\n",
            "Saved: ./firm_info_601_700.xlsx\n",
            "\n",
            "============================================================\n",
            "Processing chunk: 701 to 800\n",
            "============================================================\n",
            "\n",
            "Total number of firms to search: 10866\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing firms: 100%|\u001b[32m██████████\u001b[0m| 100/100 [13:58<00:00,  8.38s/it]\n",
            "Processing firms: 100%|\u001b[32m██████████\u001b[0m| 13/13 [01:21<00:00,  6.30s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "Summary:\n",
            "Successfully captured: 9\n",
            "Not found: 104\n",
            "Errors: 0\n",
            "Success rate: 9.0%\n",
            "==================================================\n",
            "\n",
            "Saved: ./firm_info_701_800.xlsx\n",
            "\n",
            "============================================================\n",
            "Processing chunk: 801 to 900\n",
            "============================================================\n",
            "\n",
            "Total number of firms to search: 10866\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing firms: 100%|\u001b[32m██████████\u001b[0m| 100/100 [13:10<00:00,  7.90s/it]\n",
            "Processing firms: 100%|\u001b[32m██████████\u001b[0m| 31/31 [03:33<00:00,  6.90s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "Summary:\n",
            "Successfully captured: 15\n",
            "Not found: 116\n",
            "Errors: 0\n",
            "Success rate: 15.0%\n",
            "==================================================\n",
            "\n",
            "Saved: ./firm_info_801_900.xlsx\n",
            "\n",
            "============================================================\n",
            "Processing chunk: 901 to 1000\n",
            "============================================================\n",
            "\n",
            "Total number of firms to search: 10866\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing firms: 100%|\u001b[32m██████████\u001b[0m| 100/100 [13:17<00:00,  7.97s/it]\n",
            "Processing firms: 100%|\u001b[32m██████████\u001b[0m| 4/4 [00:21<00:00,  5.34s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "Summary:\n",
            "Successfully captured: 11\n",
            "Not found: 93\n",
            "Errors: 0\n",
            "Success rate: 11.0%\n",
            "==================================================\n",
            "\n",
            "Saved: ./firm_info_901_1000.xlsx\n",
            "\n",
            "============================================================\n",
            "Processing chunk: 1001 to 1100\n",
            "============================================================\n",
            "\n",
            "Total number of firms to search: 10866\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing firms: 100%|\u001b[32m██████████\u001b[0m| 100/100 [11:55<00:00,  7.16s/it]\n",
            "Processing firms: 100%|\u001b[32m██████████\u001b[0m| 4/4 [00:25<00:00,  6.42s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "Summary:\n",
            "Successfully captured: 7\n",
            "Not found: 97\n",
            "Errors: 0\n",
            "Success rate: 7.0%\n",
            "==================================================\n",
            "\n",
            "Saved: ./firm_info_1001_1100.xlsx\n",
            "\n",
            "============================================================\n",
            "Processing chunk: 1101 to 1200\n",
            "============================================================\n",
            "\n",
            "Total number of firms to search: 10866\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing firms: 100%|\u001b[32m██████████\u001b[0m| 100/100 [11:57<00:00,  7.17s/it]\n",
            "Processing firms: 100%|\u001b[32m██████████\u001b[0m| 38/38 [04:31<00:00,  7.15s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "Summary:\n",
            "Successfully captured: 9\n",
            "Not found: 129\n",
            "Errors: 0\n",
            "Success rate: 9.0%\n",
            "==================================================\n",
            "\n",
            "Saved: ./firm_info_1101_1200.xlsx\n",
            "\n",
            "============================================================\n",
            "Processing chunk: 1201 to 1300\n",
            "============================================================\n",
            "\n",
            "Total number of firms to search: 10866\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing firms: 100%|\u001b[32m██████████\u001b[0m| 100/100 [12:19<00:00,  7.40s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "Summary:\n",
            "Successfully captured: 9\n",
            "Not found: 91\n",
            "Errors: 0\n",
            "Success rate: 9.0%\n",
            "==================================================\n",
            "\n",
            "Saved: ./firm_info_1201_1300.xlsx\n",
            "\n",
            "============================================================\n",
            "Processing chunk: 1301 to 1400\n",
            "============================================================\n",
            "\n",
            "Total number of firms to search: 10866\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing firms: 100%|\u001b[32m██████████\u001b[0m| 100/100 [11:54<00:00,  7.14s/it]\n",
            "Processing firms: 100%|\u001b[32m██████████\u001b[0m| 16/16 [02:03<00:00,  7.73s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "Summary:\n",
            "Successfully captured: 8\n",
            "Not found: 108\n",
            "Errors: 0\n",
            "Success rate: 8.0%\n",
            "==================================================\n",
            "\n",
            "Saved: ./firm_info_1301_1400.xlsx\n",
            "\n",
            "============================================================\n",
            "Processing chunk: 1401 to 1500\n",
            "============================================================\n",
            "\n",
            "Total number of firms to search: 10866\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing firms: 100%|\u001b[32m██████████\u001b[0m| 100/100 [13:06<00:00,  7.87s/it]\n",
            "Processing firms: 100%|\u001b[32m██████████\u001b[0m| 1/1 [00:06<00:00,  6.46s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "Summary:\n",
            "Successfully captured: 13\n",
            "Not found: 88\n",
            "Errors: 0\n",
            "Success rate: 13.0%\n",
            "==================================================\n",
            "\n",
            "Saved: ./firm_info_1401_1500.xlsx\n",
            "\n",
            "============================================================\n",
            "Processing chunk: 1501 to 1600\n",
            "============================================================\n",
            "\n",
            "Total number of firms to search: 10866\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing firms: 100%|\u001b[32m██████████\u001b[0m| 100/100 [13:22<00:00,  8.02s/it]\n",
            "Processing firms: 100%|\u001b[32m██████████\u001b[0m| 1/1 [00:06<00:00,  6.20s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "Summary:\n",
            "Successfully captured: 15\n",
            "Not found: 86\n",
            "Errors: 0\n",
            "Success rate: 15.0%\n",
            "==================================================\n",
            "\n",
            "Saved: ./firm_info_1501_1600.xlsx\n",
            "\n",
            "============================================================\n",
            "Processing chunk: 1601 to 1700\n",
            "============================================================\n",
            "\n",
            "Total number of firms to search: 10866\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing firms: 100%|\u001b[32m██████████\u001b[0m| 100/100 [14:03<00:00,  8.43s/it]\n",
            "Processing firms: 100%|\u001b[32m██████████\u001b[0m| 18/18 [02:36<00:00,  8.70s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "Summary:\n",
            "Successfully captured: 17\n",
            "Not found: 101\n",
            "Errors: 0\n",
            "Success rate: 17.0%\n",
            "==================================================\n",
            "\n",
            "Saved: ./firm_info_1601_1700.xlsx\n",
            "\n",
            "============================================================\n",
            "Processing chunk: 1701 to 1800\n",
            "============================================================\n",
            "\n",
            "Total number of firms to search: 10866\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing firms: 100%|\u001b[32m██████████\u001b[0m| 100/100 [12:57<00:00,  7.78s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "Summary:\n",
            "Successfully captured: 11\n",
            "Not found: 89\n",
            "Errors: 0\n",
            "Success rate: 11.0%\n",
            "==================================================\n",
            "\n",
            "Saved: ./firm_info_1701_1800.xlsx\n",
            "\n",
            "============================================================\n",
            "Processing chunk: 1801 to 1900\n",
            "============================================================\n",
            "\n",
            "Total number of firms to search: 10866\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing firms: 100%|\u001b[32m██████████\u001b[0m| 100/100 [13:28<00:00,  8.09s/it]\n",
            "Processing firms: 100%|\u001b[32m██████████\u001b[0m| 8/8 [01:21<00:00, 10.22s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "Summary:\n",
            "Successfully captured: 16\n",
            "Not found: 92\n",
            "Errors: 0\n",
            "Success rate: 16.0%\n",
            "==================================================\n",
            "\n",
            "Saved: ./firm_info_1801_1900.xlsx\n",
            "\n",
            "============================================================\n",
            "Processing chunk: 1901 to 2000\n",
            "============================================================\n",
            "\n",
            "Total number of firms to search: 10866\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing firms: 100%|\u001b[32m██████████\u001b[0m| 100/100 [14:18<00:00,  8.58s/it]\n",
            "Processing firms: 100%|\u001b[32m██████████\u001b[0m| 7/7 [00:44<00:00,  6.29s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "Summary:\n",
            "Successfully captured: 7\n",
            "Not found: 100\n",
            "Errors: 0\n",
            "Success rate: 7.0%\n",
            "==================================================\n",
            "\n",
            "Saved: ./firm_info_1901_2000.xlsx\n",
            "\n",
            "============================================================\n",
            "FINAL SUMMARY:\n",
            "Total firms captured: 219/2000\n",
            "Overall Accuracy: 10.95%\n",
            "Processing complete!\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PREVIOUS: processing(1, 500, '95628633CBBB63E551F16A5EF4BF02E0', '58736055', 'high_prob_firms.xlsx', 100)"
      ],
      "metadata": {
        "id": "F9t3yh6zNA-p"
      },
      "id": "F9t3yh6zNA-p",
      "execution_count": 6,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python [conda env:base] *",
      "language": "python",
      "name": "conda-base-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}